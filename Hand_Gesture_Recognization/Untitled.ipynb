{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf488b95-8e46-4a66-a806-8fa587633069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gestures = [\n",
    "\"like\", \"dislike\", \"peace\", \"one\", \"fist\", \"Hello\", \"Love you\"\n",
    "]\n",
    "\n",
    "# Create directories for each gesture\n",
    "for gesture in gestures:\n",
    "    if not os.path.exists(gesture):\n",
    "        os.makedirs(gesture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5601761-ff1b-43fe-9997-68a2d93a4a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press the corresponding number key for each gesture to start capturing images:\n",
      "1 - like\n",
      "2 - dislike\n",
      "3 - peace\n",
      "4 - one\n",
      "5 - fist\n",
      "6 - Hello\n",
      "7 - Love you\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# List of gestures\n",
    "\n",
    "gestures = [\n",
    "\"like\", \"dislike\", \"peace\", \"one\", \"fist\", \"Hello\", \"Love you\"\n",
    "]\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 512\n",
    "resize_height = 512\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "current_gesture = None\n",
    "image_count = 0\n",
    "\n",
    "print(\"Press the corresponding number key for each gesture to start capturing images:\")\n",
    "for i, gesture in enumerate(gestures):\n",
    "    print(f\"{i + 1} - {gesture}\")\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display instructions on the frame\n",
    "    cv2.putText(frame, \"Press 'q' to quit.\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    if current_gesture is not None:\n",
    "        cv2.putText(frame, f\"Current Gesture: {current_gesture}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f\"Image Count: {image_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif ord('1') <= key <= ord('9'):\n",
    "        index = key - ord('1')\n",
    "        if index < len(gestures):\n",
    "            current_gesture = gestures[index]\n",
    "            image_count = 0\n",
    "    elif ord('a') <= key <= ord('f'):\n",
    "        index = ord('9') - ord('0') + (key - ord('a')) + 1\n",
    "        if index < len(gestures):\n",
    "            current_gesture = gestures[index]\n",
    "            image_count = 0\n",
    "\n",
    "    if current_gesture is not None:\n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "\n",
    "        # Save the image\n",
    "        image_path = os.path.join(current_gesture, f\"{image_count}.jpg\")\n",
    "        cv2.imwrite(image_path, resized_frame)\n",
    "        image_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849b02b9-9eb2-4b25-b160-b57546e8bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4905 images for training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 128\n",
    "resize_height = 128\n",
    "\n",
    "# List of gestures\n",
    "gestures = [\n",
    "    \"call\", \"dislike\", \"first\", \"four\", \"like\", \"mute\", \"ok\", \"palm\",\n",
    "    \"peace\", \"peace_inverted\", \"rock\", \"stop\", \"stop_inverted\", \"three\",\n",
    "    \"two_up\", \"two_inverted\"\n",
    "]\n",
    "\n",
    "# Load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "images, labels = load_images_from_folder(\".\")\n",
    "\n",
    "# Normalize the images\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = to_categorical(labels, num_classes=len(gestures))\n",
    "\n",
    "print(f\"Loaded {images.shape[0]} images for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9055f8d4-c6bd-404d-ae6b-a7c26b8cf726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3924 samples\n",
      "Validation set: 981 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79c68f2e-f2c7-4253-a43a-59d8ff3cc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 298ms/step - accuracy: 0.3221 - loss: 1.8986 - val_accuracy: 0.9755 - val_loss: 0.0965\n",
      "Epoch 2/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 259ms/step - accuracy: 0.9263 - loss: 0.2191 - val_accuracy: 0.9990 - val_loss: 0.0105\n",
      "Epoch 3/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 260ms/step - accuracy: 0.9698 - loss: 0.0828 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 4/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 245ms/step - accuracy: 0.9880 - loss: 0.0371 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
      "Epoch 5/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 246ms/step - accuracy: 0.9903 - loss: 0.0269 - val_accuracy: 1.0000 - val_loss: 4.5463e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to create a binary mask\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Assume the largest contour is the hand\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        mask = np.zeros_like(gray)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Apply mask to original image\n",
    "        fg_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return fg_image\n",
    "    else:\n",
    "        # Return the original image if no contour is found\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3480f71-2845-4a5b-a224-970c4095ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 5.4075e-04\n",
      "Validation Loss: 0.0004546341369859874\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Background subtraction\n",
    "    back_sub = cv2.createBackgroundSubtractorMOG2()\n",
    "    fg_mask = back_sub.apply(gray)\n",
    "\n",
    "    # Apply mask to the original image\n",
    "    fg_image = cv2.bitwise_and(img, img, mask=fg_mask)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(fg_image, (resize_width, resize_height))\n",
    "\n",
    "    return resized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9659a138-4da8-4e2e-89cf-213e6d5f3d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: .\\call\n",
      "Loading images from: .\\dislike\n",
      "Loading images from: .\\first\n",
      "Loading images from: .\\four\n",
      "Loading images from: .\\like\n",
      "Loading images from: .\\mute\n",
      "Loading images from: .\\ok\n",
      "Loading images from: .\\palm\n",
      "Loading images from: .\\peace\n",
      "Loading images from: .\\peace_inverted\n",
      "Loading images from: .\\rock\n",
      "Loading images from: .\\stop\n",
      "Loading images from: .\\stop_inverted\n",
      "Loading images from: .\\three\n",
      "Loading images from: .\\two_up\n",
      "Loading images from: .\\two_inverted\n",
      "Loaded 4905 images for training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 128\n",
    "resize_height = 128\n",
    "\n",
    "# List of gestures\n",
    "gestures = [\n",
    "    \"call\", \"dislike\", \"first\", \"four\", \"like\", \"mute\", \"ok\", \"palm\",\n",
    "    \"peace\", \"peace_inverted\", \"rock\", \"stop\", \"stop_inverted\", \"three\",\n",
    "    \"two_up\", \"two_inverted\"\n",
    "]\n",
    "\n",
    "# Load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            print(f\"Loading images from: {gesture_folder}\")\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "                else:\n",
    "                    print(f\"Failed to load image: {img_path}\")\n",
    "        else:\n",
    "            print(f\"Gesture folder not found: {gesture_folder}\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \".\"  # Change this to the actual path if different\n",
    "images, labels = load_images_from_folder(dataset_path)\n",
    "\n",
    "# Check if images and labels were loaded\n",
    "if images.size == 0:\n",
    "    print(\"No images were loaded. Please check the dataset path and folder structure.\")\n",
    "else:\n",
    "    # Normalize the images\n",
    "    images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    labels = to_categorical(labels, num_classes=len(gestures))\n",
    "\n",
    "    print(f\"Loaded {images.shape[0]} images for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "288b6b5d-1070-48da-b33e-72c71605ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3924 samples\n",
      "Validation set: 981 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "914883ef-9040-464a-9dcc-bd6d204ae5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_24               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_25               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_26               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_27               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32768</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_24               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_25               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_26               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_27               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m4,194,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m2,064\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,485,296</span> (17.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,485,296\u001b[0m (17.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,484,400</span> (17.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,484,400\u001b[0m (17.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "INPUT_SHAPE = (128, 128, 3)  # Change to RGB input shape\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Convolutional Layer 3\n",
    "model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten and Dense Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(gestures), activation='softmax'))  # Output layer with the number of gesture classes\n",
    "\n",
    "# Compile the model\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=METRICS)\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9eda483a-a8df-4db9-8781-eb457ef9ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 2s/step - accuracy: 0.3456 - loss: 4.3523 - precision: 0.5046 - recall: 0.2439 - val_accuracy: 0.0928 - val_loss: 18.1882 - val_precision: 0.0907 - val_recall: 0.0897\n",
      "Epoch 2/5\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - accuracy: 0.5000 - loss: 1.7148 - precision: 0.8000 - recall: 0.2500 - val_accuracy: 0.0856 - val_loss: 18.3088 - val_precision: 0.0865 - val_recall: 0.0856\n",
      "Epoch 3/5\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 1s/step - accuracy: 0.5594 - loss: 1.2426 - precision: 0.7836 - recall: 0.4332 - val_accuracy: 0.1600 - val_loss: 9.7341 - val_precision: 0.1618 - val_recall: 0.1590\n",
      "Epoch 4/5\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - accuracy: 0.7188 - loss: 0.9906 - precision: 0.8636 - recall: 0.5938 - val_accuracy: 0.1081 - val_loss: 8.5405 - val_precision: 0.1024 - val_recall: 0.1009\n",
      "Epoch 5/5\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.6708 - loss: 0.8559 - precision: 0.7925 - recall: 0.5820 - val_accuracy: 0.4577 - val_loss: 3.7226 - val_precision: 0.4677 - val_recall: 0.4506\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "batch_size = 32\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "train_generator = data_generator.flow(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "# model.save('gesture_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5aa29817-9781-42a2-a2d1-b101dc7bdaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gesture_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48a622d0-0b61-48d4-afd2-37bdd44a0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Background subtraction\n",
    "    back_sub = cv2.createBackgroundSubtractorMOG2()\n",
    "    fg_mask = back_sub.apply(gray)\n",
    "\n",
    "    # Apply mask to the original image\n",
    "    fg_image = cv2.bitwise_and(img, img, mask=fg_mask)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(fg_image, (resize_width, resize_height))\n",
    "\n",
    "    return resized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41bb42af-181c-4467-b65b-4bab7226fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to create a binary mask\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Assume the largest contour is the hand\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        mask = np.zeros_like(gray)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Apply mask to original image\n",
    "        fg_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return fg_image\n",
    "    else:\n",
    "        # Return the original image if no contour is found\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67a98998-8627-4419-a1e4-a0231135cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = preprocess_image(img)\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22ca89cc-6629-45bf-a6dc-ab9096a01b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4905 images for training.\n",
      "Training set: 3924 samples\n",
      "Validation set: 981 samples\n",
      "Epoch 1/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 245ms/step - accuracy: 0.4809 - loss: 1.5364 - val_accuracy: 0.9888 - val_loss: 0.0325\n",
      "Epoch 2/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 238ms/step - accuracy: 0.9736 - loss: 0.0879 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
      "Epoch 3/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 239ms/step - accuracy: 0.9867 - loss: 0.0356 - val_accuracy: 0.9980 - val_loss: 0.0059\n",
      "Epoch 4/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 239ms/step - accuracy: 0.9909 - loss: 0.0242 - val_accuracy: 1.0000 - val_loss: 6.1311e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 229ms/step - accuracy: 0.9984 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 6.6076e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 5.2729e-04\n",
      "Validation Loss: 0.0006607609102502465\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 128\n",
    "resize_height = 128\n",
    "\n",
    "# List of gestures\n",
    "gestures = [\n",
    "    \"call\", \"dislike\", \"first\", \"four\", \"like\", \"mute\", \"ok\", \"palm\",\n",
    "    \"peace\", \"peace_inverted\", \"rock\", \"stop\", \"stop_inverted\", \"three\",\n",
    "    \"two_up\", \"two_inverted\"\n",
    "]\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to create a binary mask\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Assume the largest contour is the hand\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        mask = np.zeros_like(gray)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Apply mask to original image\n",
    "        fg_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return fg_image\n",
    "    else:\n",
    "        # Return the original image if no contour is found\n",
    "        return img\n",
    "\n",
    "# Load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = preprocess_image(img)\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "images, labels = load_images_from_folder(\".\")\n",
    "\n",
    "# Normalize the images\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = to_categorical(labels, num_classes=len(gestures))\n",
    "\n",
    "print(f\"Loaded {images.shape[0]} images for training.\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(resize_width, resize_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(gestures), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the model\n",
    "model.save('gesture_recognition_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e89439-1f32-4aa6-b4e9-4d4c6ca6efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5243 images for training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 128\n",
    "resize_height = 128\n",
    "\n",
    "# List of gestures\n",
    "gestures = [\n",
    "    \"call_me\", \"fingers_crossed\", \"okay\", \"paper\", \"peace\", \"rock\", \"rock_on\", \"scissor\", \"thumbs\", \"up\",\n",
    "    \"two_up\", \"two_inverted\"\n",
    "]\n",
    "\n",
    "# Load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "images, labels = load_images_from_folder(\"images/.\")\n",
    "\n",
    "# Normalize the images\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = to_categorical(labels, num_classes=len(gestures))\n",
    "\n",
    "print(f\"Loaded {images.shape[0]} images for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa0a6cd0-c514-420d-a04a-1a63d3c5dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[4110])\n",
    "print(labels[4110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de2bba34-7c1c-40c0-9611-595960290cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4194 samples\n",
      "Validation set: 1049 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2296af29-ddb7-4217-85a2-265695cf2161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMYLS\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 306ms/step - accuracy: 0.2836 - loss: 1.9772 - val_accuracy: 0.8561 - val_loss: 0.5548\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/131\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 251ms/step - accuracy: 0.6875 - loss: 0.9831"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python312\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6875 - loss: 0.9831 - val_accuracy: 0.8608 - val_loss: 0.5390\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 336ms/step - accuracy: 0.7134 - loss: 0.8334 - val_accuracy: 0.9285 - val_loss: 0.2528\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7500 - loss: 0.7327 - val_accuracy: 0.9361 - val_loss: 0.2531\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 306ms/step - accuracy: 0.7908 - loss: 0.6020 - val_accuracy: 0.9523 - val_loss: 0.1817\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7812 - loss: 0.6065 - val_accuracy: 0.9542 - val_loss: 0.1779\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 300ms/step - accuracy: 0.8375 - loss: 0.4946 - val_accuracy: 0.9542 - val_loss: 0.1473\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9062 - loss: 0.3814 - val_accuracy: 0.9571 - val_loss: 0.1468\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 287ms/step - accuracy: 0.8603 - loss: 0.4289 - val_accuracy: 0.8990 - val_loss: 0.2714\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8125 - loss: 0.3571 - val_accuracy: 0.9218 - val_loss: 0.2255\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9185 - loss: 0.2318\n",
      "Validation Loss: 0.22551093995571136\n",
      "Validation Accuracy: 0.9218302965164185\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For data augmentation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(resize_width, resize_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(gestures), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "batch_size = 32\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "train_generator = data_generator.flow(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "# Fit the model\n",
    "r = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "390ad24d-24bf-40a4-83c5-0fb50bad779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save('gesture_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ac147-3aa8-4645-b702-a37ae7ecd067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f7ffa-da0c-40c6-bf0d-361cd82b7e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634b7c3-d370-4a99-a302-c5e4ca29a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166631f6-f3f2-454c-929b-d1c6d83210ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d26de1-2d50-4418-af85-78faa89849f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb58b1-0e18-40a6-8265-c0ea64416ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b04500-6589-4ed2-b68e-d5771c1471e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dead727-eb20-4104-aae0-82ec92798c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Path to the folder containing the images\n",
    "folder_path = \"D:/X/AI/Internships/Prodigy InfoTech/Tasks/4 Hand Gesture Recognition/peace\"\n",
    "\n",
    "# Define the target size for resizing\n",
    "target_width = 128\n",
    "target_height = 128\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(image_path, target_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        resized_image = cv2.resize(image, target_size)\n",
    "        cv2.imwrite(image_path, resized_image)\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        # Get the full path of the image\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Resize the image\n",
    "        resize_image(file_path, (target_width, target_height))\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7e8c33e-0f7f-485a-a55e-0971beda472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3927 images for training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the size to which images will be resized\n",
    "resize_width = 128\n",
    "resize_height = 128\n",
    "\n",
    "# List of gestures\n",
    "gestures = [\n",
    "\"like\", \"dislike\", \"peace\", \"one\", \"fist\", \"Hello\", \"Love you\"\n",
    "]\n",
    "\n",
    "# Load images and labels\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for gesture_index, gesture in enumerate(gestures):\n",
    "        gesture_folder = os.path.join(folder, gesture)\n",
    "        if os.path.exists(gesture_folder):\n",
    "            for filename in os.listdir(gesture_folder):\n",
    "                img_path = os.path.join(gesture_folder, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (resize_width, resize_height))\n",
    "                    images.append(img)\n",
    "                    labels.append(gesture_index)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "images, labels = load_images_from_folder(\".\")\n",
    "\n",
    "# Normalize the images\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = to_categorical(labels, num_classes=len(gestures))\n",
    "\n",
    "print(f\"Loaded {images.shape[0]} images for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "490946dd-283b-4fcc-820f-e2a870f43e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3141 samples\n",
      "Validation set: 786 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bb5f4a2-09de-46c6-81cd-d3e7991db448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to create a binary mask\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Assume the largest contour is the hand\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        mask = np.zeros_like(gray)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Apply mask to original image\n",
    "        fg_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return fg_image\n",
    "    else:\n",
    "        # Return the original image if no contour is found\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d99d2e0c-e59d-4ad2-a753-22cc04562e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMYLS\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For data augmentation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(resize_width, resize_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(gestures), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the model\n",
    "# model.save('gesture_recognition_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(f\"Validation Loss: {val_loss}\")\n",
    "# print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5726f6b1-21b6-4718-a7e3-163befa4dfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 368ms/step - accuracy: 0.8438 - loss: 0.4536 - val_accuracy: 0.9478 - val_loss: 0.1786\n",
      "Epoch 2/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8438 - loss: 0.4193 - val_accuracy: 0.9338 - val_loss: 0.2144\n",
      "Epoch 3/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 328ms/step - accuracy: 0.8369 - loss: 0.4675 - val_accuracy: 0.9275 - val_loss: 0.2032\n",
      "Epoch 4/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7812 - loss: 0.4153 - val_accuracy: 0.9275 - val_loss: 0.1979\n",
      "Epoch 5/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 307ms/step - accuracy: 0.8764 - loss: 0.3322 - val_accuracy: 0.9962 - val_loss: 0.0794\n",
      "Epoch 6/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8438 - loss: 0.4386 - val_accuracy: 0.9860 - val_loss: 0.0842\n",
      "Epoch 7/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 324ms/step - accuracy: 0.9112 - loss: 0.2718 - val_accuracy: 0.9567 - val_loss: 0.1425\n",
      "Epoch 8/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8750 - loss: 0.3410 - val_accuracy: 0.9949 - val_loss: 0.1001\n",
      "Epoch 9/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 348ms/step - accuracy: 0.9051 - loss: 0.2838 - val_accuracy: 0.9898 - val_loss: 0.0522\n",
      "Epoch 10/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9688 - loss: 0.1277 - val_accuracy: 0.9898 - val_loss: 0.0448\n",
      "Epoch 11/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 313ms/step - accuracy: 0.9249 - loss: 0.2212 - val_accuracy: 0.9885 - val_loss: 0.0614\n",
      "Epoch 12/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8750 - loss: 0.2887 - val_accuracy: 0.9885 - val_loss: 0.0962\n",
      "Epoch 13/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.9231 - loss: 0.2084 - val_accuracy: 0.9949 - val_loss: 0.0632\n",
      "Epoch 14/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9062 - loss: 0.2201 - val_accuracy: 0.9949 - val_loss: 0.0622\n",
      "Epoch 15/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.9307 - loss: 0.1969 - val_accuracy: 1.0000 - val_loss: 0.0220\n",
      "Epoch 16/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.4668 - val_accuracy: 1.0000 - val_loss: 0.0225\n",
      "Epoch 17/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.8849 - loss: 0.3536 - val_accuracy: 0.9949 - val_loss: 0.0206\n",
      "Epoch 18/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9688 - loss: 0.0745 - val_accuracy: 0.9949 - val_loss: 0.0198\n",
      "Epoch 19/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9364 - loss: 0.1907 - val_accuracy: 0.9924 - val_loss: 0.0439\n",
      "Epoch 20/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8750 - loss: 0.2483 - val_accuracy: 0.9924 - val_loss: 0.0395\n",
      "Epoch 21/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 282ms/step - accuracy: 0.9409 - loss: 0.1799 - val_accuracy: 1.0000 - val_loss: 0.0133\n",
      "Epoch 22/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7812 - loss: 0.3923 - val_accuracy: 1.0000 - val_loss: 0.0113\n",
      "Epoch 23/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 280ms/step - accuracy: 0.9455 - loss: 0.1537 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 24/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9688 - loss: 0.0806 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 25/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 278ms/step - accuracy: 0.9524 - loss: 0.1491 - val_accuracy: 1.0000 - val_loss: 0.0108\n",
      "Epoch 26/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: 0.1314 - val_accuracy: 1.0000 - val_loss: 0.0094\n",
      "Epoch 27/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9495 - loss: 0.1281 - val_accuracy: 1.0000 - val_loss: 0.0048\n",
      "Epoch 28/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0596 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "Epoch 29/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 282ms/step - accuracy: 0.9340 - loss: 0.1957 - val_accuracy: 1.0000 - val_loss: 0.0066\n",
      "Epoch 30/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: 0.1407 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
      "Epoch 31/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9623 - loss: 0.1100 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
      "Epoch 32/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0434 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
      "Epoch 33/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9606 - loss: 0.1119 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
      "Epoch 34/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0537 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
      "Epoch 35/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 280ms/step - accuracy: 0.9580 - loss: 0.1130 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "Epoch 36/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.1258 - val_accuracy: 1.0000 - val_loss: 0.0124\n",
      "Epoch 37/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.9197 - loss: 0.2382 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 38/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.0802 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
      "Epoch 39/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9599 - loss: 0.1038 - val_accuracy: 0.9835 - val_loss: 0.0504\n",
      "Epoch 40/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8750 - loss: 0.3052 - val_accuracy: 0.9975 - val_loss: 0.0270\n",
      "Epoch 41/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 284ms/step - accuracy: 0.9620 - loss: 0.1050 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
      "Epoch 42/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
      "Epoch 43/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 281ms/step - accuracy: 0.9428 - loss: 0.1601 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 44/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9688 - loss: 0.0965 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
      "Epoch 45/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.9693 - loss: 0.0768 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 46/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9688 - loss: 0.0484 - val_accuracy: 1.0000 - val_loss: 0.0049\n",
      "Epoch 47/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 284ms/step - accuracy: 0.9419 - loss: 0.1811 - val_accuracy: 1.0000 - val_loss: 0.0133\n",
      "Epoch 48/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9062 - loss: 0.2928 - val_accuracy: 1.0000 - val_loss: 0.0160\n",
      "Epoch 49/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 279ms/step - accuracy: 0.9448 - loss: 0.1742 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 50/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8750 - loss: 0.2126 - val_accuracy: 1.0000 - val_loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "batch_size = 32\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "train_generator = data_generator.flow(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "# model.save('gesture_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0c4e68f-1d48-4730-ad7a-d7375ef81a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gesture_recognition_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
